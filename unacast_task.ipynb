{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1JkNJIVk34NH57CBOV3hX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayRayKing/unacast_visit/blob/main/unacast_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions\n",
        "\n",
        "1. Upload the daily_data folder to the content folder\n",
        "2. Run script"
      ],
      "metadata": {
        "id": "NgwZ5okST7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCL0minT5e04",
        "outputId": "ee8dc2ef-cc93-4cea-f95d-cebf49500b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "venue_id             0\n",
            "visitor_id           0\n",
            "visit_start_time     0\n",
            "visit_end_time      19\n",
            "venue_type           3\n",
            "dtype: int64\n",
            "venue_id            object\n",
            "visitor_id          object\n",
            "visit_start_time    object\n",
            "visit_end_time      object\n",
            "venue_type          object\n",
            "dtype: object\n",
            "       venue_id visitor_id            visit_start_time  \\\n",
            "count      2182       2182                        2182   \n",
            "unique       15       1087                         783   \n",
            "top       UN004      V0205  2024-11-15 09:00:00.000000   \n",
            "freq        348          6                           9   \n",
            "\n",
            "                    visit_end_time  venue_type  \n",
            "count                         2163        2179  \n",
            "unique                        2152           4  \n",
            "top     2024-11-15 15:43:35.462349  university  \n",
            "freq                             2        1599  \n"
          ]
        }
      ],
      "source": [
        "#Step 1 Load and review data. With possible data potentials\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# locally saved path\n",
        "daily_data_folder = '/content/daily_data/'\n",
        "\n",
        "# csv path for all files\n",
        "file_paths = glob.glob(f\"{daily_data_folder}/*.csv\")\n",
        "\n",
        "first_file = pd.read_csv(file_paths[0])\n",
        "\n",
        "# potential data issues\n",
        "\n",
        "# missing values\n",
        "print(first_file.isnull().sum())\n",
        "\n",
        "# data type\n",
        "print(first_file.dtypes)\n",
        "\n",
        "# Additional other data checks\n",
        "# valid all venue_id/visitor_ids are real IDs against current db.\n",
        "# Time consistency. Start date not greater thte end date\n",
        "# duplicate data sets --  possibility of true duplicates?\n",
        "\n",
        "# Basic statistics\n",
        "print(first_file.describe(include='all'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "file_path = '/content/daily_data/20241028.csv'\n",
        "\n",
        "def process_daily_file_df(file_path):\n",
        "    \"\"\"\n",
        "    Processes a daily file of venue visitation data, cleans it, computes metrics,\n",
        "    and appends the results to a summary CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): Path to the daily CSV file to process.\n",
        "    - summary_file_name (str): Path to the summary CSV file to save results. Default: 'daily_visitation_summary.csv'.\n",
        "    \"\"\"\n",
        "\n",
        "    daily_data = pd.read_csv(file_path)\n",
        "\n",
        "    # drop rows with missing or invalid data\n",
        "    daily_data.dropna(inplace=True)\n",
        "    # an alternative would be go enrich the missing/valid data on case by case assumptions; of course this is with given business context\n",
        "    # e.g. missing venue types can be extraploited from the venue_ID (with some mapping) - can use historicals to define mapping\n",
        "    # e.g. visiting_end time - can end at the 23:59:59 - true end of day, ( assuming no staying overnight issues)\n",
        "\n",
        "\n",
        "    # convert columns to their proper data types\n",
        "    daily_data['visit_start_time'] = pd.to_datetime(daily_data['visit_start_time'], errors='coerce')\n",
        "    daily_data['visit_end_time'] = pd.to_datetime(daily_data['visit_end_time'], errors='coerce')\n",
        "    daily_data['venue_id'] = daily_data['venue_id'].astype('string')\n",
        "    daily_data['visitor_id'] = daily_data['visitor_id'].astype('string')\n",
        "    daily_data['venue_type'] = daily_data['venue_type'].astype('string')\n",
        "\n",
        "    # calc metrics\n",
        "    results = daily_data.groupby('venue_id').agg(\n",
        "        visitor_count_unique=('visitor_id', 'nunique'),\n",
        "        visitor_count_total=('visitor_id', 'count')\n",
        "    ).reset_index()\n",
        "\n",
        "    # date column (extracted from the file name)\n",
        "    processing_date = os.path.basename(file_path).split('.')[0]\n",
        "    results['date'] = processing_date\n",
        "\n",
        "    # simple average\n",
        "    overall_average = results['visitor_count_total'].mean()\n",
        "\n",
        "    # overall average as the prediction for all venues\n",
        "    results['visitor_count_total_prediction'] = overall_average\n",
        "\n",
        "\n",
        "    # reorder columns to match the output schema\n",
        "    results = results[['date', 'venue_id', 'visitor_count_unique', 'visitor_count_total', 'visitor_count_total_prediction']]\n",
        "\n",
        "    summary_file_name = 'daily_visitation_summary.csv'\n",
        "    # save the results to the summary file\n",
        "    if not os.path.exists(summary_file_name):\n",
        "        results.to_csv(summary_file_name, index=False)\n",
        "    else:\n",
        "        results.to_csv(summary_file_name, mode='a', header=False, index=False)\n",
        "\n",
        "    print(f\"Processed and saved data for {processing_date}\")\n",
        "\n",
        "\n",
        "# Process to for call one file.\n",
        "process_daily_file_df(file_path)\n",
        "\n",
        "\n",
        "# # Loop through all files..\n",
        "# for file_path in file_paths:\n",
        "    # process_daily_file_df(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmd4NHqy-E9s",
        "outputId": "596c1063-369a-4272-a484-8899db56aab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for 20241115\n",
            "Processed and saved data for 20241107\n",
            "Processed and saved data for 20241028\n",
            "Processed and saved data for 20241108\n",
            "Processed and saved data for 20241110\n",
            "Processed and saved data for 20241031\n",
            "Processed and saved data for 20241104\n",
            "Processed and saved data for 20241109\n",
            "Processed and saved data for 20241111\n",
            "Processed and saved data for 20241029\n",
            "Processed and saved data for 20241102\n",
            "Processed and saved data for 20241105\n",
            "Processed and saved data for 20241106\n",
            "Processed and saved data for 20241117\n",
            "Processed and saved data for 20241113\n",
            "Processed and saved data for 20241116\n",
            "Processed and saved data for 20241112\n",
            "Processed and saved data for 20241030\n",
            "Processed and saved data for 20241114\n",
            "Processed and saved data for 20241103\n",
            "Processed and saved data for 20241101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "1. How would you scale this pipeline to handle:\n",
        " - 20 million locations of 20 million users\n",
        "  \n",
        "  This would be focused on distributed processing (most likely spark). From there is a matter of optimization for the processing of the data. Ensuring partitions keys are set correctly so that the workload can be properly distributed.\n",
        "\n",
        " - Real-time updates\n",
        "\n",
        "  Messaging/queueing systems (Redis). These systems will allow for real time processing of data as they come in, and queue up if there is a sudden overflow of data. It will continue to process and queue up if there are sudden spikes.\n",
        "\n",
        "2. How would you store the data?\n",
        "\n",
        "Data lake style on cloud provider BLOB systems. For GCP thats google cloud storage (Buckets).\n",
        "\n",
        "3. How would you monitor data quality?\n",
        "\n",
        "The would consist of strong testings throughout the pipeline. It'll can be done with python tests (great expectations), sql (dbt based), or even external observability tools (monte carlos).\n",
        "\n",
        "4. What would your daily orchestration look like?\n",
        "\n",
        "On an airflow platform, most likely consistenting of simple tasks extract -> transform -> load. With retries and error handling where needed."
      ],
      "metadata": {
        "id": "bCmnZZH4RkMU"
      }
    }
  ]
}